### JAVA 线上故障排查

>   线上故障主要会包括cpu、磁盘、内存以及网络问题，而大多数故障可能会包含不止一个层面的问题，所以进行排查时候尽量四个方面依次排查一遍。
>
>   同时例如jstack、jmap等工具也是不囿于一个方面的问题的，基本上出问题就是df、free、top 三连，然后依次jstack、jmap伺候，具体问题具体分析即可。



### CPU

>   一般来讲我们首先会排查cpu方面的问题。cpu异常往往还是比较好定位的。原因包括业务逻辑问题(死循环)、频繁gc以及上下文切换过多。而最常见的往往是业务逻辑(或者框架逻辑)导致的，可以使用jstack来分析对应的堆栈情况。

#### 使用jstack分析cpu问题

我们先用ps命令找到对应进程的pid(如果你有好几个目标进程，可以先用top看一下哪个占用比较高)。

接着用top -H -p pid来找到cpu使用率比较高的一些线程

![image-20210219100149488](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910014949.png)

使用`top -H -p PID` 定位占用CPU较高的线程

![image-20210219100814767](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910081414.png)

然后将占用最高的pid转换为16进制 `printf '%x\n' PID` 得到 `NID`

![image-20210219100346455](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910034646.png)

接着直接在jstack中找到相应的堆栈信息`jstack PID | grep NID -B 3 -A 50` 或者使用简化命令 `jstack PID | grep $(printf '%x' NID) -B 3 -A 50`

![image-20210219101043271](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910104343.png)

可以看到我们已经找到了`nid`为`0x2f6d`的堆栈信息，接着只要仔细分析一番即可。

可以使用`jstack PID >> jstack.dump` 将线程信息输出到文件

当然更常见的是我们对整个jstack文件进行分析，通常我们会比较关注`WAITING`和`TIMED_WAITING`的部分，`BLOCKED`就不用说了。我们可以使用命令`cat jstack.log | grep "java.lang.Thread.State" | sort -nr | uniq -c`来对jstack的状态有一个整体的把握，如果`WAITING`之类的特别多，那么多半是有问题啦。

![image-20210219101443179](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910144343.png)

#### 频繁GC

当然我们还是会使用jstack来分析问题，但有时候我们可以先确定下gc是不是太频繁，使用`jstat -gc pid 1000`命令来对gc分代变化情况进行观察，1000表示采样间隔(ms)，S0C/S1C、S0U/S1U、EC/EU、OC/OU、MC/MU分别代表两个Survivor区、Eden区、老年代、元数据区的容量和使用量。YGC/YGT、FGC/FGCT、GCT则代表YoungGc、FullGc的耗时和次数以及总耗时。如果看到gc比较频繁，再针对gc方面做进一步分析。

![image-20210219102131155](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910213131.png)

#### 上次文切换

针对频繁上下文问题，我们可以使用`vmstat PID`命令来进行查看

![image-20210219102645817](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910264545.png)

cs(context switch)一列则代表了上下文切换的次数。

如果我们希望对特定的pid进行监控那么可以使用 `pidstat -w PID` 命令，cswch和nvcswch表示自愿及非自愿切换。

![image-20210219103159267](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910315959.png)



### 磁盘

磁盘问题和cpu一样是属于比较基础的。首先是磁盘空间方面，我们直接使用df -lh来查看文件系统状态

![image-20210219103259784](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910325959.png)

更多时候，磁盘问题还是性能上的问题。我们可以通过iostat -d -k -x来进行分析

![image-20210219103509566](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/202102191035099.png)

最后一列%util可以看到每块磁盘写入的程度，而rrqpm/s以及wrqm/s分别表示读写速度，一般就能帮助定位到具体哪块磁盘出现问题了。

找到pid之后就可以看这个进程具体的读写情况cat /proc/pid/io

![image-20210219104818929](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910481818.png)

我们还可以通过lsof命令来确定具体的文件读写情况`lsof -p pid`

![image-20210219105134029](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910513434.png)

#### 打开的文件过多异常

[问题描述与解决方案](./../centos/centos文件句柄.md)

### 内存

内存问题排查起来相对比CPU麻烦一些，场景也比较多。主要包括OOM、GC问题和堆外内存。一般来讲，我们会先用free命令先来检查一发内存的各种情况。

![image-20210219105349459](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910534949.png)

#### 堆内内存

内存问题大多还都是堆内内存问题。表象上主要分为OOM和StackOverflow。

##### OOM

JMV中的内存不足，OOM大致可以分为以下几种：

>   ```java
>   Exception in thread "main" java.lang.OutOfMemoryError: unable to create new native thread
>   ```
>
>   这个意思是没有足够的内存空间给线程分配java栈，基本上还是线程池代码写的有问题，比如说忘记shutdown，所以说应该首先从代码层面来寻找问题，使用jstack或者jmap。如果一切都正常，JVM方面可以通过指定Xss来减少单个thread stack的大小。
>
>   另外也可以在系统层面，可以通过修改/etc/security/limits.confnofile和nproc来增大os对线程的限制

>   ```java
>   Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
>   ```
>
>   这个意思是堆的内存占用已经达到-Xmx设置的最大值，应该是最常见的OOM错误了。解决思路仍然是先应该在代码中找，怀疑存在内存泄漏，通过jstack和jmap去定位问题。如果说一切都正常，才需要通过调整Xmx的值来扩大内存。

>   ```java
>   Caused by: java.lang.OutOfMemoryError: Meta space
>   ```
>
>   这个意思是元数据区的内存占用已经达到XX:MaxMetaspaceSize设置的最大值，排查思路和上面的一致，参数方面可以通过XX:MaxPermSize来进行调整(这里就不说1.8以前的永久代了)。

##### Stack Overflow

栈内存溢出，这个大家见到也比较多。

>   ```java
>   Exception in thread "main" java.lang.StackOverflowError
>   ```
>
>   表示线程栈需要的内存大于Xss值，同样也是先进行排查，参数方面通过Xss来调整，但调整的太大可能又会引起OOM。

#### 使用JMAP定位代码内存泄漏

上述关于OOM和StackOverflow的代码排查方面，我们一般使用`JMAPjmap -dump:format=b,file=filename PID`来导出dump文件

通过mat(Eclipse Memory Analysis Tools)导入dump文件进行分析，内存泄漏问题一般我们直接选Leak Suspects即可，mat给出了内存泄漏的建议。另外也可以选择Top Consumers来查看最大对象报告。和线程相关的问题可以选择thread overview进行分析。除此之外就是选择Histogram类概览来自己慢慢分析，大家可以搜搜mat的相关教程。

![img](https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhq5hUg5NpzQCMvSzUOmdUKlMmiaullovFzgibZG49AgDRiayry9xSpXcqibJ7Kh9HHEuOeH9sEegKooZQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

日常开发中，代码产生内存泄漏是比较常见的事，并且比较隐蔽，需要开发者更加关注细节。比如说每次请求都new对象，导致大量重复创建对象；进行文件流操作但未正确关闭；手动不当触发gc；ByteBuffer缓存分配不合理等都会造成代码OOM。

另一方面，我们可以在启动参数中指定`-XX:+HeapDumpOnOutOfMemoryError`来保存OOM时的dump文件。

#### GC问题和线程

gc问题除了影响cpu也会影响内存，排查思路也是一致的。一般先使用jstat来查看分代变化情况，比如youngGC或者fullGC次数是不是太多呀；EU、OU等指标增长是不是异常呀等。

线程的话太多而且不被及时gc也会引发oom，大部分就是之前说的unable to create new native thread。除了jstack细细分析dump文件外，我们一般先会看下总体线程，通过pstreee -p pid |wc -l。

![image-20210219110030992](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911003131.png)

或者直接通过查看/proc/pid/task的数量即为线程数量。

![image-20210219110220454](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911022020.png)

#### 堆外内存

如果碰到堆外内存溢出，那可真是太不幸了。首先堆外内存溢出表现就是物理常驻内存增长快，报错的话视使用方式都不确定，如果由于使用Netty导致的，那错误日志里可能会出现OutOfDirectMemoryError错误，如果直接是DirectByteBuffer，那会报`OutOfMemoryError: Direct buffer memory`。

堆外内存溢出往往是和NIO的使用相关，一般我们先通过pmap来查看下进程占用的内存情况pmap -x pid | sort -rn -k3 | head -30，这段意思是查看对应pid倒序前30大的内存段。这边可以再一段时间后再跑一次命令看看内存增长情况，或者和正常机器比较可疑的内存段在哪里。

![image-20210219110345360](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911034545.png)

我们如果确定有可疑的内存端，需要通过gdb来分析gdb --batch --pid {pid} -ex "dump memory filename.dump {内存起始地址} {内存起始地址+内存块大小}"



### GC问题



### 网络





-   https://mp.weixin.qq.com/s/OEuFYxrUBWdLIJhZAjjBsQ