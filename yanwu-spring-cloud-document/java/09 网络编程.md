### ServerSocket
#### socket：是一个四元组
四元组：ClientIP:ClientPort对应ServerIP:ServerPort

通过窗口机制解决拥塞

***
## IO
### 文件系统IO
kernel：内核
VFS：虚拟文件系统
FD：文件描述符：打开的文件所持有的
inode：在虚拟文件系统当中，每个文件在打开的时候都会有一个inode号
pagecache：页缓存，内存中对数据的缓存，默认4k大小，两个程序读取同一个文件时
dirty：脏

如果程序读取了磁盘中的文件，那么这些文件会缓存到pagecache中，当程序修改了某一个pagecache中的数据时，那么该pagecache会被标记为dirty的，最终通过flush写到磁盘中去。脏页写入磁盘的方式有两种：
- 程序等待内核自动刷新
- 程序调用内核刷新

bytebuffer：想写的时候compact，想读的时候flip

### 内存和IO关系

### 网络IO

#### BIO模型
同步阻塞IO，它的阻塞是在accept函数和recv函数这两个地方，BIO的服务端执行顺序为：
- 服务端启动后，会创建一个socket，并且随机返回一个文件描述符（fd-A）
- 然后将这个文件描述符（fd-A）和server的端口bind
- bind后内核会进入listen状态监听这个文件描述符（通过 __netstat -natp__ 命令可以查看到该LISTEN状态）
- 监听建立完后服务会进行系统调用，通过accept函数等待客户端链接，此时进入 __阻塞状态__，等待客户端链接
- 当有新的客户端建立链接进来后，会给对应的客户端分配一个新的文件描述符（fd-B）
- 分配完文件描述符（fd-B）后服务端会clone一个线程来通过recv函数来读取该文件描述符的内容，此时clone出来的线程也会进入 __阻塞状态__

![clipboard](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/20200628140248.png)

##### 弊端
- 阻塞（booking）

#### NIO模型
同步非阻塞IO，NIO的诞生主要是为了解决BIO的阻塞问题。使用BIO模型时，当有新的连接进来时，会clone出新的线程出来读取对应的socket中的信息，所以当连接数很大的时候，会有很多的系统资源浪费在：线程调度、用户态内核态切换、内存空间等等的资源消耗上，服务端和客户端通过configureBlocking(falg)设置为是否阻塞【true：阻塞；false：不阻塞】，NIO的服务端执行顺序为：
- 服务端启动后，会创建一个socket，并且随机返回一个文件描述符（fd-A）
- 然后将这个文件描述符（fd-A）和server的端口bind
- bind后内核会进入listen状态监听这个文件描述符（通过 __netstat -natp__ 命令可以查看到该LISTEN状态）
- 由于通过configureBlocking(false)将服务端设置为非阻塞，所以此时通过服务进行系统调用通过accept函数等待客户端链接时，__不会被阻塞__，而是会根据场景直接返回，不停的循环检查accept函数的返回值判断是否有新的客户端进来：
    - 有客户端连接进来：给对应的客户端分配一个新的文件描述苻（fd-B），并且通过configureBlocking(false)将对应的客户端也设置为 __非阻塞__
    - 没有客户端连接进来：OS：返回 -1；JDK：返回null
- 循环通过recv函数依次读取所有客户端判断是否有发送数据过来，进行相应的业务处理

##### channel
一个channel代表一个文件

##### buffer
channel读写文件的单位

![clipboard](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/20200628140308.png)

position <= limit <= capacity
###### capacity
buffer的容量
###### limit
buffer可供读写的临界点
###### position
当前读写的位置

##### mmap

##### selector
NIO的核心，将channel注册到selector

###### 函数
- 通过register()函数将channel注册到selector
- 通过keys()函数返回所有注册上来的channel的key的集合
- 通过selectionKey()函数返回准备好可操作的channel的key的集合，selectionKey集合中的channel的key使用完后必须使用Iterator.remove()函数进行销毁，不然会导致重复消费

![clipboard](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/20200628140338.png)

##### 弊端
- 当并发非常大的时候，比如一次循环到accept有10个客户端同时发送连接请求过来，那么有可能只有第一个连接建立成功，其他的连接请求会被丢掉
- 如果有10K个连接的时候，每循环recv一次的时间复杂度是O(10000)，此时很多调用是无意义的

#### 多路复用器模型
所谓的多路复用的意思是指：通过一次系统调用获取所有的IO的状态

多路复用器只负责返回对应的IO连接的状态，然后由程序去对有状态的那些IO去读写

其实 __NIO和select、pool都是要遍历所有IO的询问状态__，但不同点在于：NIO是由 __服务去循环，这种方式会频繁的产生用户态到内核态的切换__；而select与poll的 __系统调用是一次用户态到内核态切换，过程中将所有的文件描述符传递给内核，内核根据这些文件描述符遍历__，修改状态

##### select
将服务所有连接所持有的文件描述符作为参数通过服务调用内核的select函数，然后select函数会返回对应有文件读写事件的发生的文件描述符，服务再去处理相对应的文件描述符的读写

###### 弊端
最多能够接受1024个文件描述符

##### poll
与select相似，区别是poll没有1024的限制

###### select & poll的弊端
- 每次调用都要重复传递所有文件描述符
- 在内核态中需要遍历所有的文件描述符

###### IO中断级别
- package
- buffer
- 轮询

###### 回调（callback）
- 根据中断向量表中间的中断号和回调执行对应的回调函数

##### epoll
在epoll之前的callback，只是完成了将网卡发来的数据走内核网络协议栈最终关联到文件描述符的buffer，所以在某一个时间如果从应用程序询问内核某一个或者某些文件描述符是否有读写事件时，会有状态返回

##### select、poll && epoll
在Java中，select、poll、epoll都是通过Selector类来实现

![clipboard](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/20200628140402.png)

##### 零拷贝

## [Netty](https://www.sohu.com/a/272879207_463994)
### 事件循环组
通过 __线程的环形队列__ 实现事件循环组，通过事件循环组实现异步操作
### 执行器
特殊的事件循环组

自己管理内存，减少上下文切换

- netty-buffer
- netty-codec