### JAVA 线上故障排查

>   线上故障主要会包括cpu、磁盘、内存以及网络问题，而大多数故障可能会包含不止一个层面的问题，所以进行排查时候尽量四个方面依次排查一遍。
>
>   同时例如jstack、jmap等工具也是不囿于一个方面的问题的，基本上出问题就是df、free、top 三连，然后依次jstack、jmap伺候，具体问题具体分析即可。



### CPU

>   一般来讲我们首先会排查cpu方面的问题。cpu异常往往还是比较好定位的。原因包括业务逻辑问题(死循环)、频繁gc以及上下文切换过多。而最常见的往往是业务逻辑(或者框架逻辑)导致的，可以使用jstack来分析对应的堆栈情况。

#### 使用jstack分析cpu问题

我们先用ps命令找到对应进程的pid(如果你有好几个目标进程，可以先用top看一下哪个占用比较高)。

接着用top -H -p pid来找到cpu使用率比较高的一些线程

![image-20210219100149488](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910014949.png)

使用`top -H -p PID` 定位占用CPU较高的线程

![image-20210219100814767](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910081414.png)

然后将占用最高的pid转换为16进制 `printf '%x\n' PID` 得到 `NID`

![image-20210219100346455](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910034646.png)

接着直接在jstack中找到相应的堆栈信息`jstack PID | grep NID -B 3 -A 50` 或者使用简化命令 `jstack PID | grep $(printf '%x' NID) -B 3 -A 50`

![image-20210219101043271](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910104343.png)

可以看到我们已经找到了`nid`为`0x2f6d`的堆栈信息，接着只要仔细分析一番即可。

可以使用`jstack PID >> jstack.dump` 将线程信息输出到文件

当然更常见的是我们对整个jstack文件进行分析，通常我们会比较关注`WAITING`和`TIMED_WAITING`的部分，`BLOCKED`就不用说了。我们可以使用命令`cat jstack.log | grep "java.lang.Thread.State" | sort -nr | uniq -c`来对jstack的状态有一个整体的把握，如果`WAITING`之类的特别多，那么多半是有问题啦。

![image-20210219101443179](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910144343.png)



#### 频繁GC

当然我们还是会使用`jstack`来分析问题，但有时候我们可以先确定下`gc`是不是太频繁，使用`jstat -gc pid 1000`命令来对`gc`分代变化情况进行观察，`1000`表示采样间隔(`ms`)，`S0C/S1C`、`S0U/S1U`、`EC/EU`、`OC/OU`、`MC/MU`分别代表两个`Survivor`区、`Eden`区、老年代、元数据区的容量和使用量。`YGC/YGT`、`FGC/FGCT`、`GCT`则代表`YoungGc`、`FullGc`的耗时和次数以及总耗时。如果看到`gc`比较频繁，再针对`gc`方面做进一步分析。

![image-20210219102131155](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910213131.png)



#### 上次文切换

针对频繁上下文问题，我们可以使用`vmstat PID`命令来进行查看

![image-20210219102645817](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910264545.png)

`cs(context switch)`一列则代表了上下文切换的次数。

如果我们希望对特定的`pid`进行监控那么可以使用 `pidstat -w PID` 命令，`cswch`和`nvcswch`表示自愿及非自愿切换。

![image-20210219103159267](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910315959.png)



### 磁盘

磁盘问题和`cpu`一样是属于比较基础的。首先是磁盘空间方面，我们直接使用`df -lh`来查看文件系统状态

![image-20210219103259784](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910325959.png)

更多时候，磁盘问题还是性能上的问题。我们可以通过`iostat -d -k -x`来进行分析

![image-20210219103509566](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/202102191035099.png)

最后一列`%util`可以看到每块磁盘写入的程度，而`rrqpm/s`以及`wrqm/s`分别表示读写速度，一般就能帮助定位到具体哪块磁盘出现问题了。

找到`pid`之后就可以看这个进程具体的读写情况`cat /proc/pid/io`

![image-20210219104818929](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910481818.png)

我们还可以通过`lsof`命令来确定具体的文件读写情况`lsof -p pid`

![image-20210219105134029](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910513434.png)

#### 打开的文件过多异常

[问题描述与解决方案](./../centos/centos文件句柄.md)



### 内存

内存问题排查起来相对比`CPU`麻烦一些，场景也比较多。主要包括`OOM`、`GC`问题和堆外内存。一般来讲，我们会先用`free`命令先来检查一发内存的各种情况。

![image-20210219105349459](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021910534949.png)

#### 堆内内存

内存问题大多还都是堆内内存问题。表象上主要分为`OOM`和`StackOverflow`。

##### OOM

`JMV`中的内存不足，`OOM`大致可以分为以下几种：

>   ```java
>   Exception in thread "main" java.lang.OutOfMemoryError: unable to create new native thread
>   ```
>
>   这个意思是没有足够的内存空间给线程分配`java`栈，基本上还是线程池代码写的有问题，比如说忘记`shutdown`，所以说应该首先从代码层面来寻找问题，使用`jstack`或者`jmap`。如果一切都正常，`JVM`方面可以通过指定`Xss`来减少单个`thread stack`的大小。
>
>   另外也可以在系统层面，可以通过修改`/etc/security/limits.confnofile`和`nproc`来增大`os`对线程的限制

>   ```java
>   Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
>   ```
>
>   这个意思是堆的内存占用已经达到`-Xmx`设置的最大值，应该是最常见的`OOM`错误了。解决思路仍然是先应该在代码中找，怀疑存在内存泄漏，通过`jstack`和`jmap`去定位问题。如果说一切都正常，才需要通过调整`Xmx`的值来扩大内存。

>   ```java
>   Caused by: java.lang.OutOfMemoryError: Meta space
>   ```
>
>   这个意思是元数据区的内存占用已经达到`XX:MaxMetaspaceSize`设置的最大值，排查思路和上面的一致，参数方面可以通过`XX:MaxPermSize`来进行调整

##### Stack Overflow

栈内存溢出，这个大家见到也比较多。

>   ```java
>   Exception in thread "main" java.lang.StackOverflowError
>   ```
>
>   表示线程栈需要的内存大于`Xss`值，同样也是先进行排查，参数方面通过`Xss`来调整，但调整的太大可能又会引起`OOM`。

#### 使用JMAP定位代码内存泄漏

上述关于`OOM`和`StackOverflow`的代码排查方面，我们一般使用`JMAPjmap -dump:format=b,file=filename PID`来导出`dump`文件

通过`mat(Eclipse Memory Analysis Tools)`导入`dump`文件进行分析，内存泄漏问题一般我们直接选`Leak Suspects`即可，`mat`给出了内存泄漏的建议。另外也可以选择`Top Consumers`来查看最大对象报告。和线程相关的问题可以选择`thread overview`进行分析。除此之外就是选择`Histogram`类概览来自己慢慢分析，大家可以搜搜mat的相关教程。

![img](https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhq5hUg5NpzQCMvSzUOmdUKlMmiaullovFzgibZG49AgDRiayry9xSpXcqibJ7Kh9HHEuOeH9sEegKooZQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

日常开发中，代码产生内存泄漏是比较常见的事，并且比较隐蔽，需要开发者更加关注细节。比如说每次请求都`new`对象，导致大量重复创建对象、进行文件流操作但未正确关闭、手动不当触发`gc`、`ByteBuffer`缓存分配不合理等都会造成代码`OOM`。

另一方面，我们可以在启动参数中指定`-XX:+HeapDumpOnOutOfMemoryError`来保存`OOM`时的`dump`文件。

#### GC问题和线程

`gc`问题除了影响`cpu`也会影响内存，排查思路也是一致的。一般先使用`jstat`来查看分代变化情况，比如`youngGC`或者`fullGC`次数是不是太多、`EU、OU`等指标增长是不是异常等。

线程的话太多而且不被及时`gc`也会引发`oom`，大部分就是之前说的`unable to create new native thread`。除了`jstack`细细分析`dump`文件外，我们一般先会看下总体线程，通过`pstreee -p pid | wc -l`。

![image-20210219110030992](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911003131.png)

或者直接通过查看`/proc/pid/task`的数量即为线程数量。

![image-20210219110220454](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911022020.png)

#### 堆外内存

如果碰到堆外内存溢出，那可真是太不幸了。首先堆外内存溢出表现就是物理常驻内存增长快，报错的话视使用方式都不确定，如果由于使用`Netty`导致的，那错误日志里可能会出现`OutOfDirectMemoryError`错误，如果直接是`DirectByteBuffer`，那会报`OutOfMemoryError: Direct buffer memory`。

堆外内存溢出往往是和`NIO`的使用相关，一般我们先通过`pmap`来查看下进程占用的内存情况`pmap -x pid | sort -rn -k3 | head -30`，这段意思是查看对应`pid`倒序前`30`大的内存段。这边可以再一段时间后再跑一次命令看看内存增长情况，或者和正常机器比较可疑的内存段在哪里。

![image-20210219110345360](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/02/2021021911034545.png)

我们如果确定有可疑的内存端，需要通过`gdb`来分析`gdb --batch --pid {pid} -ex "dump memory filename.dump {内存起始地址} {内存起始地址+内存块大小}"`



### GC问题



### 网络





-   https://mp.weixin.qq.com/s/OEuFYxrUBWdLIJhZAjjBsQ