### 问题描述

![image-20210607112745406](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060711274545.png)

事情起源于某个无所事事的早晨，那天，我正开开心心无忧无虑的泡在知乎摸鱼的时候，突然接到了阿里云的告警，身为打工人的我，只好暂时告别了我的摸鱼人的生涯，开始了漫长的BUG查找生涯......

#### JVM配置

首先，看看服务的JVM参数

![image-20210607094543921](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060709454343.png)

```bash
13702 /home/admin/bird-ops/target/bird-ops/bird-ops-0.0.1-SNAPSHOT.jar -Xms2g -Xmx2g -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -Xmn1024m -XX:MaxDirectMemorySize=512m -XX:SurvivorRatio=10 -XX:+UseConcMarkSweepGC -XX:CMSMaxAbortablePrecleanTime=5000 -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly -XX:+ExplicitGCInvokesConcurrent -Dsun.rmi.dgc.server.gcInterval=2592000000 -Dsun.rmi.dgc.client.gcInterval=2592000000 -XX:ParallelGCThreads=2 -Xloggc:/home/admin/logs/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/admin/logs/java.hprof -Djava.awt.headless=true -Dsun.net.client.defaultConnectTimeout=10000 -Dsun.net.client.defaultReadTimeout=30000 -DJM.LOG.PATH=/home/admin/logs -DJM.SNAPSHOT.PATH=/home/admin/snapshots -Dfile.encoding=UTF-8 -Dhsf.publish.delayed=true -Dproject.name=bird-ops -Dpandora.boot.wait=true -Dlog4j.defaultInitOverride=true -Dserver.port=7001 -Dmanagement.port=7002 -Dspring.profiles.active=ol -Dapp.location=/home/admin/bird-ops/target/bird-ops
```

#### 查看线程状况

然后在遇到cpu或内存问题时，首先要看看是那个线程在做精做怪

通过 `top -Hp {PID}` & ` jstack {PID}| grep $(printf '%x' {TID}) -A50` 命令，可以知道，现在占用CPU和内存最高的几个线程都是在执行GC

![file-read-5413](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/202106071412044.png)

#### 查看GC日志

查看服务的GC日志，果然发现服务在疯狂的左右摇摆

![image-20210607141525824](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714152525.png)

#### 分析GC日志

将`GC`日志`down`下来，使用[GC日志分析工具](https://gceasy.io/)分析日志，可以得到以下几个结论：

-   `metaSpace`空间分配不合理，可以适当减小

    ![image-20210607141937600](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714193737.png)

-   已经因为`CMS`的并发标记、并发清除阶段找成了`STW`问题，并长达`3`个小时

    ![image-20210607142534257](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714253434.png)

    ![image-20210607142224062](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714222424.png)

-   得到之所以不停的`GC`的原因：是因为内存不足导致的。__（什么鬼？？？我堆的配置可是有`2G`啊！！！）__

    ![image-20210607142616374](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714261616.png)

#### dump

果断地将服务`dump`下来，分析问题

```shell
./jmap -dump:live,format=b,file=/tmp/20210607-10910dump.hprof 10910
```

使用[MAT](http://www.eclipse.org/mat/)分析一波，发现有个神奇的对象，它占用了高达`900M`的内存。`WTF`？？？

![image-20210607142936065](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714293636.png)

让我们看看这个对象金屋里到底藏了什么娇，这么不舍得释放，结果一看，我差点儿吐血，这`TM`是什么鬼`SQL`，一个`SQL`语句能到`2M`，而且这个大对象中这中`2M`大小的`SQL`高达一两百个。

![image-20210607143251126](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714325252.png)

找到对应的开发一问，才知道，这个`SQL`是一个批量操作的`SQL`，同时批量操作了`3000`多条数据。。。。。好吧，牛逼

可是批量操作也能理解，但是为什么这里面有一千多条`SQL`在内存中呢？而且还一直都在？？这个`DruidDataSourceWrapper`中的`JdbcDataSourceStat`是什么鬼？？

结果查看源码才知道，由于`druid`有个功能，叫做__监控统计__，当开启了该功能时，`druid`会将SQL缓存起来，以便在查看监控的时候展示`SQL`。。。

![image-20210607145258644](https://typroa12138.oss-cn-hangzhou.aliyuncs.com/image/2021/06/2021060714525858.png)

#### 结论

经过苦逼的定位后，得出以下三个结论：

-   调整`metaSpace`内存，建议调整为：`128m`，最大：`256m`
-   调整大`SQL`的拼装，将大`SQL`拆成小`SQL`执行，将大事务拆成小事务执行
-   关闭`druid`的监控统计功能，避免`druid`持有`SQL`不释放导致内存不足引起的`GC`



### 引用

才知道，被`druid`的监控支配过的不止我一个，还有很多人。

https://www.jianshu.com/p/fb37ab115121